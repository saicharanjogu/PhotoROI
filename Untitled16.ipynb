{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saicharanjogu/PhotoROI/blob/main/Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EmzuHbvXtX1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install retina-face opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sc1lSaWaetp",
        "outputId": "130e71fa-4e2a-43bd-f7a8-456b91eb0024"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting retina-face\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from retina-face) (2.0.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from retina-face) (5.2.0)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from retina-face) (11.3.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from retina-face) (2.19.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->retina-face) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (3.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=3.10.1->retina-face) (2.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.1.2)\n",
            "Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: retina-face\n",
            "Successfully installed retina-face-0.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stage2_cluster_faces.py\n",
        "\n",
        "## Note : Dont run this cell ##\n",
        "\n",
        "''' The basic internal working of this code is mentioned in below cell'''\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from retinaface import RetinaFace\n",
        "from deepface import DeepFace\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def run_face_clustering(image_folder='event_images', output_csv='face_data.csv', cluster_folder='output_clusters'):\n",
        "    \"\"\"\n",
        "    Detects, embeds, and clusters all faces from an image folder.\n",
        "    Saves a CSV with all face data and sample images for each cluster.\n",
        "    \"\"\"\n",
        "    print(\"--- Running Stage 2: Face Detection and Clustering ---\")\n",
        "\n",
        "    # checks if the folder exists if exists then it will delete that folder.\n",
        "    if os.path.exists(cluster_folder):\n",
        "        shutil.rmtree(cluster_folder)\n",
        "\n",
        "    # create a new folder - cluster_folder\n",
        "    os.makedirs(cluster_folder, exist_ok=True)\n",
        "\n",
        "    all_faces_data = []\n",
        "\n",
        "    # Loop through all the images. and does face detection. how it works is detailed in next cell.\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            continue\n",
        "\n",
        "        filepath = os.path.join(image_folder, filename)\n",
        "        print(f\"Processing {filename}...\")\n",
        "        try:\n",
        "            image = cv2.imread(filepath)\n",
        "\n",
        "            # face detection(part 1- explain in next cell)\n",
        "            detected_faces = RetinaFace.detect_faces(filepath)\n",
        "            if not isinstance(detected_faces, dict): continue\n",
        "\n",
        "            for face_id, data in detected_faces.items():\n",
        "                x1, y1, x2, y2 = data['facial_area']\n",
        "                face_img = image[y1:y2, x1:x2]\n",
        "\n",
        "                # This is the second part of stage 2 where we will take the faces we got from face detector and convert them into 512D vector(that is now the faces are represented in an array of length 512)\n",
        "                embedding_obj = DeepFace.represent(face_img, model_name='ArcFace', enforce_detection=False)\n",
        "                embedding = embedding_obj[0][\"embedding\"]\n",
        "\n",
        "                all_faces_data.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"facial_area\": data['facial_area'],\n",
        "                    \"embedding\": embedding\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"  -> Could not process {filename}: {e}\")\n",
        "\n",
        "    if not all_faces_data:\n",
        "        print(\"\\nNo faces were detected in any images. Exiting.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(all_faces_data)\n",
        "    embeddings = np.array(df['embedding'].tolist())\n",
        "\n",
        "    # Perform clustering (3rd part of stage 2)\n",
        "    # here we will identify the person using unsupervised learning(machine learning - clusterning)\n",
        "    # You can tune the distance_threshold. Lower values create more, smaller clusters.\n",
        "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=3.701, linkage='average')\n",
        "    df['cluster'] = clustering.fit_predict(embeddings)\n",
        "\n",
        "    print(f\"\\nClustering complete. Found {df['cluster'].nunique()} unique people.\")\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Full face data saved to '{output_csv}'\")\n",
        "\n",
        "    # Save sample faces for manual labeling (additional - here we will take a sampe which can represent that whole group, in our case in simple terms individual person)\n",
        "    for cluster_id in sorted(df['cluster'].unique()):\n",
        "        if cluster_id == -1: continue # Skip noise points\n",
        "\n",
        "        cluster_df = df[df['cluster'] == cluster_id]\n",
        "        cluster_embeddings = np.array(cluster_df['embedding'].tolist())\n",
        "\n",
        "        centroid = np.mean(cluster_embeddings, axis=0)\n",
        "        distances = cdist(cluster_embeddings, centroid.reshape(1, -1))\n",
        "        closest_index_in_cluster = distances.argmin()\n",
        "\n",
        "        representative_face = cluster_df.iloc[closest_index_in_cluster]\n",
        "        img = cv2.imread(os.path.join(image_folder, representative_face['filename']))\n",
        "        x1, y1, x2, y2 = representative_face['facial_area']\n",
        "        face_crop = img[y1:y2, x1:x2]\n",
        "\n",
        "        cv2.imwrite(os.path.join(cluster_folder, f'cluster_{cluster_id}.jpg'), face_crop)\n",
        "\n",
        "    print(f\"Sample faces for each person saved to the '{cluster_folder}' folder.\")\n",
        "    print(\"--- Stage 2 Complete ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_face_clustering()"
      ],
      "metadata": {
        "id": "OJgfM9-rwN9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# here we will explain the code to identify the faces and we will draw bounding box around that face and lastly we will save the image"
      ],
      "metadata": {
        "id": "4IomSDaiyP0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z90H7QV8FRNJ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from retinaface import RetinaFace\n",
        "from deepface import DeepFace\n",
        "\n",
        "# Path to your image\n",
        "image_path = 'RA1_3428.jpg' # ðŸ‘ˆ Replace with the path to your image\n",
        "\n",
        "# Read the image using OpenCV\n",
        "try:\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Error: Unable to read the image at '{image_path}'. Please check the file path.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    exit()\n",
        "\n",
        "print(\"Detecting faces... ðŸ§\")\n",
        "\n",
        "# The 'detect_faces' function returns a dictionary where keys are face identifiers\n",
        "# and values contain the bounding box ('facial_area') and other landmarks.\n",
        "try:\n",
        "    faces = RetinaFace.detect_faces(image_path)\n",
        "\n",
        "    # Check if any faces were detected\n",
        "    if not isinstance(faces, dict):\n",
        "        print(\"No faces detected in the image. ðŸ¤”\")\n",
        "    else:\n",
        "        # Iterate over all detected faces\n",
        "        for face_id in faces:\n",
        "            # Get the bounding box coordinates\n",
        "            facial_area = faces[face_id]['facial_area']\n",
        "\n",
        "            # Extract the top-left and bottom-right coordinates\n",
        "            x1, y1, x2, y2 = facial_area[0], facial_area[1], facial_area[2], facial_area[3]\n",
        "            face_img = image[y1:y2, x1:x2]\n",
        "            # Draw a rectangle around the detected face\n",
        "            # The color is in BGR format (Blue, Green, Red)\n",
        "            # Here, we use green (0, 255, 0) with a thickness of 2 pixels.\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # convert face image to vector/array\n",
        "            embedding_obj = DeepFace.represent(face_img, model_name='ArcFace', enforce_detection=False)\n",
        "            embedding = embedding_obj[0][\"embedding\"]\n",
        "            print(embedding)\n",
        "\n",
        "        print(f\"Success! Found {len(faces)} face(s). âœ…\")\n",
        "\n",
        "        # Define the output image path\n",
        "        output_path = 'output_with_faces.jpg'\n",
        "\n",
        "        # Save the image with the bounding boxes\n",
        "        cv2.imwrite(output_path, image)\n",
        "\n",
        "        print(f\"Image saved to '{output_path}'.\")\n",
        "\n",
        "        # To display the image in a window (optional)\n",
        "        # cv2.imshow(\"Detected Faces\", image)\n",
        "        # cv2.waitKey(0)\n",
        "        # cv2.destroyAllWindows()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during face detection: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# since We are done with 2nd stage no lets go to 3rd stage"
      ],
      "metadata": {
        "id": "c8yte1ME2Kw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stage3_scoring_engine.py\n",
        "\n",
        "## Dont run this code its just a repeat of code from .py file ##\n",
        "''' a sample of how it works is provided in below cells'''\n",
        "\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from math import hypot\n",
        "\n",
        "# --- HELPER FUNCTIONS FOR QUALITY METRICS (No changes here) ---\n",
        "def get_eye_aspect_ratio(eye_points, facial_landmarks):\n",
        "    p1 = (facial_landmarks.part(eye_points[0]).x, facial_landmarks.part(eye_points[0]).y)\n",
        "    p2 = (facial_landmarks.part(eye_points[1]).x, facial_landmarks.part(eye_points[1]).y)\n",
        "    p3 = (facial_landmarks.part(eye_points[2]).x, facial_landmarks.part(eye_points[2]).y)\n",
        "    p4 = (facial_landmarks.part(eye_points[3]).x, facial_landmarks.part(eye_points[3]).y)\n",
        "    p5 = (facial_landmarks.part(eye_points[4]).x, facial_landmarks.part(eye_points[4]).y)\n",
        "    p6 = (facial_landmarks.part(eye_points[5]).x, facial_landmarks.part(eye_points[5]).y)\n",
        "    vertical_dist1 = hypot(p2[0] - p6[0], p2[1] - p6[1])\n",
        "    vertical_dist2 = hypot(p3[0] - p5[0], p3[1] - p5[1])\n",
        "    horizontal_dist = hypot(p1[0] - p4[0], p1[1] - p4[1])\n",
        "    if horizontal_dist == 0: return 0.0\n",
        "    return (vertical_dist1 + vertical_dist2) / (2.0 * horizontal_dist)\n",
        "\n",
        "def get_smile_score(mouth_points, facial_landmarks):\n",
        "    p_corner_left = (facial_landmarks.part(mouth_points[0]).x, facial_landmarks.part(mouth_points[0]).y)\n",
        "    p_corner_right = (facial_landmarks.part(mouth_points[1]).x, facial_landmarks.part(mouth_points[1]).y)\n",
        "    p_jaw_left = (facial_landmarks.part(2).x, facial_landmarks.part(2).y)\n",
        "    p_jaw_right = (facial_landmarks.part(14).x, facial_landmarks.part(14).y)\n",
        "    mouth_width = hypot(p_corner_left[0] - p_corner_right[0], p_corner_left[1] - p_corner_right[1])\n",
        "    jaw_width = hypot(p_jaw_left[0] - p_jaw_right[0], p_jaw_left[1] - p_jaw_right[1])\n",
        "    if jaw_width == 0: return 0.0\n",
        "    return mouth_width / jaw_width\n",
        "\n",
        "def check_sharpness(image_gray, threshold=100.0):\n",
        "    laplacian_var = cv2.Laplacian(image_gray, cv2.CV_64F).var()\n",
        "    return 1 if laplacian_var > threshold else 0\n",
        "\n",
        "# --- THE MAIN SCORING FUNCTION (REVISED) ---\n",
        "def score_photo(image_path, people_data, predictor):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None: return None, \"Bad Photo ðŸ‘Ž\"\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    except Exception:\n",
        "        return None, \"Bad Photo ðŸ‘Ž\"\n",
        "\n",
        "    if check_sharpness(gray_image, threshold=60.0) == 0:\n",
        "        return None, \"Bad Photo ðŸ‘Ž\"\n",
        "\n",
        "    category_metrics = {'main': [], 'important': [], 'other': []}\n",
        "\n",
        "    for person_id, data in people_data.items():\n",
        "        category = data['category']\n",
        "        x1, y1, x2, y2 = data['facial_area']\n",
        "\n",
        "        dlib_rect = dlib.rectangle(int(x1), int(y1), int(x2), int(y2))\n",
        "        landmarks = predictor(gray_image, dlib_rect)\n",
        "\n",
        "        left_ear = get_eye_aspect_ratio([36, 37, 38, 39, 40, 41], landmarks)\n",
        "        right_ear = get_eye_aspect_ratio([42, 43, 44, 45, 46, 47], landmarks)\n",
        "        eye_score = 1 if (left_ear + right_ear) / 2.0 > 0.20 else 0\n",
        "\n",
        "        smile_score = 1 if get_smile_score([48, 54], landmarks) > 0.38 else 0\n",
        "        sharpness_score = check_sharpness(gray_image[y1:y2, x1:x2], threshold=90.0)\n",
        "\n",
        "        category_metrics[category].append({\n",
        "            \"eye\": eye_score,\n",
        "            \"smile\": smile_score,\n",
        "            \"focus\": sharpness_score\n",
        "        })\n",
        "\n",
        "\n",
        "    detailed_scores = {}\n",
        "    category_length={}\n",
        "    for category, metrics_list in category_metrics.items():\n",
        "\n",
        "        category_length[category]=len(metrics_list)\n",
        "\n",
        "        if not metrics_list:\n",
        "\n",
        "            detailed_scores[category] = {\"eye\": \"N/A\", \"smile\": \"N/A\", \"focus\": \"N/A\"}\n",
        "        else:\n",
        "\n",
        "            all_eyes = 1 if all(m['eye'] == 1 for m in metrics_list) else 0\n",
        "            all_smiles = 1 if any(m['smile'] == 1 for m in metrics_list) else 0\n",
        "            all_focus = 1 if any(m['focus'] == 1 for m in metrics_list) else 0\n",
        "\n",
        "            detailed_scores[category] = {\n",
        "                \"eye\": all_eyes,\n",
        "                \"smile\": all_smiles,\n",
        "                \"focus\": all_focus\n",
        "            }\n",
        "\n",
        "\n",
        "    return detailed_scores, category_length"
      ],
      "metadata": {
        "id": "90xTuqrx2RhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This is sample code for 3rd stage where we check eyes,smile, and focus.\n",
        "\n",
        "First lets start with maths behind it.\n",
        "\n",
        "Of course. Here is a detailed explanation of the mathematics and computer vision concepts behind each of those functions.\n",
        "\n",
        "-----\n",
        "\n",
        "### \\#\\# 1. Math Behind `get_eye_aspect_ratio`\n",
        "\n",
        "**The Concept:** The goal is to find a single number that represents how \"open\" an eye is. The **Eye Aspect Ratio (EAR)** achieves this by comparing the height of the eye to its width. When an eye closes, its height collapses to almost zero while its width stays nearly the same, causing the ratio to drop sharply.\n",
        "\n",
        "-----\n",
        "\n",
        "**The Calculation:**\n",
        "\n",
        "The function uses 6 specific facial landmark points that outline the eye, as shown below:\n",
        "\n",
        "1.  **Distance Measurement:** The code uses the `hypot` function, which calculates the standard **Euclidean distance** between two points $(x\\_1, y\\_1)$ and $(x\\_2, y\\_2)$. This is based on the Pythagorean theorem:\n",
        "    $Distance = \\\\sqrt{(x\\_2 - x\\_1)^2 + (y\\_2 - y\\_1)^2}$\n",
        "\n",
        "2.  **Height and Width:**\n",
        "\n",
        "      * `vertical_dist1` and `vertical_dist2` measure the two vertical distances across the open eye (the \"height\").\n",
        "      * `horizontal_dist` measures the horizontal distance between the corners of the eye (the \"width\").\n",
        "\n",
        "3.  **The Formula:** The final ratio is calculated with the formula:\n",
        "    $$EAR = \\frac{||p_2 - p_6|| + ||p_3 - p_5||}{2 \\cdot ||p_1 - p_4||}$$\n",
        "    Where $||p\\_a - p\\_b||$ represents the Euclidean distance between points $p\\_a$ and $p\\_b$.\n",
        "\n",
        "This formula is essentially:\n",
        "$$EAR = \\frac{Average Height}{Width}$$\n",
        "Because the width is relatively constant, any change in the EAR value is almost entirely due to the eye opening or closing.\n",
        "\n",
        "-----\n",
        "\n",
        "### \\#\\# 2. Math Behind `get_smile_score`\n",
        "\n",
        "**The Concept:** The goal is to create a score that indicates a smile, regardless of the person's face size or their distance from the camera. A smile is characterized by the mouth widening horizontally.\n",
        "\n",
        "-----\n",
        "\n",
        "**The Calculation:**\n",
        "\n",
        "1.  **Measure Mouth Width:** The code first calculates the Euclidean distance between the corners of the mouth (landmarks 48 and 54).\n",
        "    $Mouth Width = ||p\\_{48} - p\\_{54}||$\n",
        "\n",
        "2.  **Normalization:** A simple mouth width measurement isn't enough, as a person with a larger face will naturally have a wider mouth. To solve this, we **normalize** the measurement by dividing it by a stable facial feature that relates to face size. The code uses the distance between two points on the jawline (landmarks 2 and 14) for this.\n",
        "    $Jaw Width = ||p\\_{2} - p\\_{14}||$\n",
        "\n",
        "3.  **The Formula:** The final score is the ratio of these two measurements.\n",
        "    $$Smile Score = \\frac{Mouth Width}{Jaw Width}$$\n",
        "    By creating this ratio, the score becomes independent of the overall scale of the face, providing a much more reliable indicator of a smile.\n",
        "\n",
        "-----\n",
        "\n",
        "### \\#\\# 3. Math Behind `check_sharpness`\n",
        "\n",
        "**The Concept:** This function determines if an image is sharp or blurry by measuring the amount of \"detail\" or \"edges\" it contains. Sharp images have many well-defined edges, while blurry images have smooth, gradual transitions.\n",
        "\n",
        "-----\n",
        "\n",
        "**The Calculation:**\n",
        "\n",
        "This is a two-step process in computer vision:\n",
        "\n",
        "1.  **The Laplacian Operator (`cv2.Laplacian`):**\n",
        "\n",
        "      * The Laplacian is a mathematical operator that measures the second derivative of an image. In simpler terms, it's highly sensitive to areas where pixel intensity changes rapidly.\n",
        "      * When applied to an image, it produces a new image where:\n",
        "          * **Edges and details** have high positive or negative values.\n",
        "          * **Smooth or flat areas** have values close to zero.\n",
        "      * Therefore, the output of the Laplacian on a sharp image will have many strong, non-zero values. The output on a blurry image will be mostly black (close to zero).\n",
        "\n",
        "2.  **Variance (`.var()`):**\n",
        "\n",
        "      * After applying the Laplacian, the code calculates the **variance** of the resulting pixel values. Variance is a statistical measure of how spread out a set of numbers is.\n",
        "      * For a **sharp image**, the Laplacian output has a wide spread of values (many high and low numbers), resulting in a **high variance**.\n",
        "      * For a **blurry image**, the Laplacian output is mostly zero, so the values are not spread out at all, resulting in a **low variance**.\n",
        "\n",
        "The function then simply checks if this calculated variance is above a certain `threshold` to decide if the image is sharp (1) or blurry (0)."
      ],
      "metadata": {
        "id": "-RvdrndF1fPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from math import hypot\n",
        "from retinaface import RetinaFace\n",
        "\n",
        "# --- 1. INITIALIZE DLIB'S LANDMARK PREDICTOR ---\n",
        "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
        "try:\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "except RuntimeError:\n",
        "    print(f\"Error: Could not find '{predictor_path}'. Make sure you ran the setup cell first.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. HELPER FUNCTIONS FOR QUALITY METRICS ---\n",
        "def get_eye_aspect_ratio(eye_points, facial_landmarks):\n",
        "    p1 = (facial_landmarks.part(eye_points[0]).x, facial_landmarks.part(eye_points[0]).y)\n",
        "    p2 = (facial_landmarks.part(eye_points[1]).x, facial_landmarks.part(eye_points[1]).y)\n",
        "    p3 = (facial_landmarks.part(eye_points[2]).x, facial_landmarks.part(eye_points[2]).y)\n",
        "    p4 = (facial_landmarks.part(eye_points[3]).x, facial_landmarks.part(eye_points[3]).y)\n",
        "    p5 = (facial_landmarks.part(eye_points[4]).x, facial_landmarks.part(eye_points[4]).y)\n",
        "    p6 = (facial_landmarks.part(eye_points[5]).x, facial_landmarks.part(eye_points[5]).y)\n",
        "    vertical_dist1 = hypot(p2[0] - p6[0], p2[1] - p6[1])\n",
        "    vertical_dist2 = hypot(p3[0] - p5[0], p3[1] - p5[1])\n",
        "    horizontal_dist = hypot(p1[0] - p4[0], p1[1] - p4[1])\n",
        "    if horizontal_dist == 0: return 0.0\n",
        "    return (vertical_dist1 + vertical_dist2) / (2.0 * horizontal_dist)\n",
        "\n",
        "def get_smile_score(mouth_points, facial_landmarks):\n",
        "    p_corner_left = (facial_landmarks.part(mouth_points[0]).x, facial_landmarks.part(mouth_points[0]).y)\n",
        "    p_corner_right = (facial_landmarks.part(mouth_points[1]).x, facial_landmarks.part(mouth_points[1]).y)\n",
        "    p_jaw_left = (facial_landmarks.part(2).x, facial_landmarks.part(2).y)\n",
        "    p_jaw_right = (facial_landmarks.part(14).x, facial_landmarks.part(14).y)\n",
        "    mouth_width = hypot(p_corner_left[0] - p_corner_right[0], p_corner_left[1] - p_corner_right[1])\n",
        "    jaw_width = hypot(p_jaw_left[0] - p_jaw_right[0], p_jaw_left[1] - p_jaw_right[1])\n",
        "    if jaw_width == 0: return 0.0\n",
        "    return mouth_width / jaw_width\n",
        "\n",
        "def check_sharpness(image_gray, threshold=100.0):\n",
        "    laplacian_var = cv2.Laplacian(image_gray, cv2.CV_64F).var()\n",
        "    return 1 if laplacian_var > threshold else 0\n",
        "\n",
        "# --- 3. MAIN LOGIC ---\n",
        "\n",
        "# â€¼ï¸ IMPORTANT: Right-click your uploaded image in the file explorer and 'Copy path'\n",
        "image_path = '/content/RA1_3428.jpg'\n",
        "\n",
        "try:\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Error: Unable to read the image at '{image_path}'.\")\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    exit()\n",
        "\n",
        "# Quick global check: if the whole image is blurry, it's bad.\n",
        "if check_sharpness(gray_image, threshold=60.0) == 0:\n",
        "    print(\"Photo rejected: Overall image is too blurry. Final Score: 5.0\")\n",
        "    exit()\n",
        "\n",
        "print(\"Detecting and categorizing faces... ðŸ§\")\n",
        "\n",
        "## this part is similar to our 2nd stage (face detection)\n",
        "try:\n",
        "    detected_faces = RetinaFace.detect_faces(image_path)\n",
        "\n",
        "    if not isinstance(detected_faces, dict):\n",
        "        print(\"No faces detected in the image. ðŸ¤”\")\n",
        "    else:\n",
        "        people_data = {}\n",
        "        for face_id, data in detected_faces.items():\n",
        "            person_number = int(face_id.split('_')[-1])\n",
        "            if person_number in [5, 6]:\n",
        "                category = 'main'\n",
        "            elif person_number in [4, 7]:\n",
        "                category = 'important'\n",
        "            else:\n",
        "                category = 'other'\n",
        "            people_data[face_id] = {'category': category, 'facial_area': data['facial_area']}\n",
        "        print(f\"Categorized {len(people_data)} faces based on detection order.\")\n",
        "\n",
        "        # from here the eye,smile and blur calculation take place.\n",
        "        category_scores = {'main': [], 'important': [], 'other': []}\n",
        "        for face_id, data in people_data.items():\n",
        "            category = data['category']\n",
        "            x1, y1, x2, y2 = data['facial_area']\n",
        "\n",
        "            dlib_rect = dlib.rectangle(int(x1), int(y1), int(x2), int(y2))\n",
        "            landmarks = predictor(gray_image, dlib_rect)\n",
        "\n",
        "            # -- Check if eyes are open or close\n",
        "            left_eye_pts = [36, 37, 38, 39, 40, 41]\n",
        "            right_eye_pts = [42, 43, 44, 45, 46, 47]\n",
        "            left_ear = get_eye_aspect_ratio(left_eye_pts, landmarks)\n",
        "            right_ear = get_eye_aspect_ratio(right_eye_pts, landmarks)\n",
        "            eye_score = 1 if (left_ear + right_ear) / 2.0 > 0.2 else 0\n",
        "\n",
        "            # -- Check if smile\n",
        "            smile_score = 1 if get_smile_score([48, 54], landmarks) > 0.38 else 0\n",
        "\n",
        "            # -- check blurr\n",
        "            sharpness_score = check_sharpness(gray_image[y1:y2, x1:x2], threshold=90.0)\n",
        "\n",
        "\n",
        "            ## everything from here is to calculate score(which we removed. But added another feature that is store above values)\n",
        "            person_score = (eye_score * 0.5) + (sharpness_score * 0.3) + (smile_score * 0.2)\n",
        "            category_scores[category].append(person_score)\n",
        "\n",
        "            color = {'main': (255, 0, 255), 'important': (255, 255, 0), 'other': (0, 255, 0)}\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), color[category], 2)\n",
        "            cv2.putText(image, f\"{category} ({face_id})\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color[category], 2)\n",
        "\n",
        "        main_avg = np.mean(category_scores['main']) if category_scores['main'] else -1\n",
        "        important_avg = np.mean(category_scores['important']) if category_scores['important'] else -1\n",
        "        other_avg = np.mean(category_scores['other']) if category_scores['other'] else -1\n",
        "\n",
        "        if main_avg == -1:\n",
        "            final_score = 10.0\n",
        "        elif main_avg < 0.6:\n",
        "            final_score = main_avg * 20\n",
        "        else:\n",
        "            if important_avg == -1: important_avg = 0.8\n",
        "            if other_avg == -1: other_avg = 0.7\n",
        "            w_main, w_imp, w_oth = 0.6, 0.25, 0.15\n",
        "            final_score = (main_avg * w_main + important_avg * w_imp + other_avg * w_oth) * 100\n",
        "\n",
        "        print(f\"\\n--- Photo Analysis Complete ---\")\n",
        "        print(f\"Main Person(s) Avg Quality: {main_avg:.2f}\")\n",
        "        print(f\"Important Person(s) Avg Quality: {important_avg:.2f}\")\n",
        "        print(f\"Other Person(s) Avg Quality: {other_avg:.2f}\")\n",
        "        print(f\"FINAL PHOTO SCORE: {final_score:.2f} âœ…\")\n",
        "\n",
        "        output_path = '/content/final_scored_output.jpg'\n",
        "        cv2.imwrite(output_path, image)\n",
        "        print(f\"\\nAnnotated image saved to '{output_path}'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo42kp-GcaQB",
        "outputId": "c1a04b44-c500-4fbe-95b2-aa81f4e2dcf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting and categorizing faces... ðŸ§\n",
            "Categorized 10 faces based on detection order.\n",
            "\n",
            "--- Photo Analysis Complete ---\n",
            "Main Person(s) Avg Quality: 0.85\n",
            "Important Person(s) Avg Quality: 0.85\n",
            "Other Person(s) Avg Quality: 1.00\n",
            "FINAL PHOTO SCORE: 87.25 âœ…\n",
            "\n",
            "Annotated image saved to '/content/final_scored_output.jpg'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is the part which combines 2nd and 3rd stage"
      ],
      "metadata": {
        "id": "QhUxkzXz5JgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main_workflow.py\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd         # For handling data in tables (DataFrames)\n",
        "import json                 # For reading the user-defined category map\n",
        "import os                   # For handling file and folder paths\n",
        "import dlib                 # For loading the facial landmark model\n",
        "from ast import literal_eval # To safely convert a string like \"[1,2,3]\" back into a list\n",
        "import numpy as np          # For numerical operations\n",
        "\n",
        "# Import the main scoring function from your other file.\n",
        "from stage3_scoring_engine import score_photo\n",
        "\n",
        "def run_main_workflow():\n",
        "    \"\"\"\n",
        "    Orchestrates the scoring and saves the detailed results to a CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize variables for the folder and file paths that will be used.\n",
        "    IMAGE_FOLDER = 'event_images'\n",
        "    DATA_FILE = 'face_data.csv'\n",
        "    CATEGORY_MAP_FILE = 'category_map.json'  # this is the input for 3rd stage a json file\n",
        "    OUTPUT_CSV_FILE = 'photo_scores_report.csv' # Define the output CSV filename\n",
        "\n",
        "    # --- Load prerequisite files ---\n",
        "    try:\n",
        "        # Load the face data from Stage 2 into a pandas DataFrame.\n",
        "        df = pd.read_csv(DATA_FILE)\n",
        "        # The 'facial_area' column is saved as a string; this converts it back into a list of numbers.\n",
        "        df['facial_area'] = df['facial_area'].apply(literal_eval)\n",
        "        # Open and load the JSON file where the user mapped cluster IDs to categories.\n",
        "        with open(CATEGORY_MAP_FILE) as f:\n",
        "            cluster_to_category_map = json.load(f)\n",
        "        # Load the dlib facial landmark model into memory.\n",
        "        dlib_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    except FileNotFoundError as e:\n",
        "        # If any file is missing, print an error and stop the program.\n",
        "        print(f\"Error: A required file is missing: {e.filename}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Starting Stage 3: Scoring All Photos ---\")\n",
        "    # Create an empty list to store the final results for each photo.\n",
        "    photo_results = []\n",
        "\n",
        "    # This pandas function groups all the detected faces by their filename.\n",
        "    # The loop will run once for each unique photo in your dataset.\n",
        "    for filename, group in df.groupby('filename'):\n",
        "        # For each photo, create an empty dictionary to hold data about the people in it.\n",
        "        people_data = {}\n",
        "        # Now, loop through each face found in this specific photo.\n",
        "        for idx, row in group.iterrows():\n",
        "            # Get the cluster ID for the current face.\n",
        "            cluster_id = str(row['cluster'])\n",
        "            # Look up that cluster ID in your map to find its category ('main', 'important', 'other').\n",
        "            category = cluster_to_category_map.get(cluster_id, 'other')\n",
        "\n",
        "            # Add this person's information (category and face location) to the dictionary for this photo.\n",
        "            people_data[f'person_{idx}'] = {\n",
        "                'category': category,\n",
        "                'facial_area': row['facial_area']\n",
        "            }\n",
        "\n",
        "        # Create the full path to the current image file.\n",
        "        full_image_path = os.path.join(IMAGE_FOLDER, filename)\n",
        "        # Call the scoring function from the other file, passing all necessary data for this photo.\n",
        "        # It returns the detailed scores and a list of people in each category.\n",
        "        detailed_scores, category_list = score_photo(full_image_path, people_data, dlib_predictor)\n",
        "\n",
        "        # Initialize an empty string to build the custom 'abc' result code.\n",
        "        output_string = ''\n",
        "        # This check ensures we only process photos that were successfully scored.\n",
        "        if detailed_scores:\n",
        "            # Loop through the main dictionary ('main', 'important', 'other').\n",
        "            for metrix in detailed_scores.values():\n",
        "                # Loop through the inner dictionary ('eye', 'smile', 'focus').\n",
        "                for val in metrix.values():\n",
        "                    # Based on the score (0, 1, or N/A), append a letter to the result string.\n",
        "                    if val == 0:\n",
        "                        value_string = 'a'\n",
        "                    elif val == 1:\n",
        "                        value_string = 'b'\n",
        "                    else:\n",
        "                        value_string = 'c'\n",
        "                    # Add the letter to the final string.\n",
        "                    output_string += value_string\n",
        "\n",
        "        # If the photo was scored successfully...\n",
        "        if detailed_scores:\n",
        "            # ...append a dictionary containing all the final data to our results list.\n",
        "            photo_results.append({\n",
        "                'photo_path': full_image_path,\n",
        "                'detailed_scores': detailed_scores,\n",
        "                'main': category_list['main'],\n",
        "                'important': category_list['important'],\n",
        "                'others': category_list['other'],\n",
        "                'total' : category_list['main'] + category_list['important'] + category_list['other'],\n",
        "                'results': output_string\n",
        "            })\n",
        "\n",
        "    # --- FINAL STEP: SAVE RESULTS TO CSV ---\n",
        "    # If no photos were scored for any reason, print a message and exit.\n",
        "    if not photo_results:\n",
        "        print(\"\\nNo photos were scored. Exiting without creating a CSV.\")\n",
        "        return\n",
        "\n",
        "    # Convert the list of result dictionaries into a pandas DataFrame.\n",
        "    results_df = pd.DataFrame(photo_results)\n",
        "\n",
        "    # Save the DataFrame to a CSV file, without the default pandas index column.\n",
        "    results_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
        "\n",
        "    print(f\"\\nâœ… Workflow complete. Detailed scores saved to '{OUTPUT_CSV_FILE}'.\")\n",
        "\n",
        "\n",
        "# This block is the entry point when you run `python main_workflow.py`\n",
        "if __name__ == '__main__':\n",
        "    # It calls the main function to start the entire process.\n",
        "    run_main_workflow()"
      ],
      "metadata": {
        "id": "o2LRNPbVz84b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "steps to do to run the whole process\n",
        "\n",
        "Here is the step-by-step process to run your complete project using Docker.\n",
        "\n",
        "-----\n",
        "\n",
        "### \\#\\# One-Time Setup\n",
        "\n",
        "1.  **Build the Docker Image:**\n",
        "    Open a terminal in your main project folder and run this command. You only need to do this once unless you change the `Dockerfile` or `requirements.txt`.\n",
        "\n",
        "    ```bash\n",
        "    docker build -t photo-sorter .\n",
        "    ```\n",
        "\n",
        "-----\n",
        "\n",
        "### \\#\\# Running the Workflow (For Each Event)\n",
        "\n",
        "1.  **Add Photos:**\n",
        "    Place the images you want to process into the `event_images/` folder.\n",
        "\n",
        "2.  **Run Stage 2 (Clustering):**\n",
        "    This command finds all the unique people in your photos.\n",
        "\n",
        "    ```bash\n",
        "    docker run --rm -v .:/app photo-sorter python stage2_cluster_faces.py\n",
        "    ```\n",
        "\n",
        "    This will create the `output_clusters/` folder and the `face_data.csv` file.\n",
        "\n",
        "3.  **Label People (Manual Step):**\n",
        "\n",
        "      * Look at the sample images in the `output_clusters/` folder.\n",
        "      * Create a file named `category_map.json`.\n",
        "      * Edit it to assign a role (`main`, `important`, `other`) to each cluster number.\n",
        "\n",
        "4.  **Run Stage 3 (Scoring):**\n",
        "    This command scores all the photos based on your labels and creates the final report.\n",
        "\n",
        "    ```bash\n",
        "    docker run --rm -v .:/app photo-sorter python main_workflow.py\n",
        "    ```\n",
        "\n",
        "5.  **Check Results:**\n",
        "    Open the newly created `photo_scores_report.csv` file to see the detailed scores for each of your photos. âœ…"
      ],
      "metadata": {
        "id": "296zxo7GC4qD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}